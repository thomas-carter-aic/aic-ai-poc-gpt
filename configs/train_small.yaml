# Minimal training config for GPT-2 small / DistilGPT-2 fine-tune (toy dataset)
model:
  hf_id: "distilgpt2"          # change to gpt2 if you prefer
  tokenizer_id: "distilgpt2"
  context_length: 512

train:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  learning_rate: 2e-4
  max_length: 256
  fp16: false
  bf16: false
  output_dir: "./outputs/distilgpt2_lora"
  logging_steps: 10
  save_total_limit: 3
  save_steps: 200

lora:
  enabled: true
  r: 4
  lora_alpha: 16
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
